{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Gostaria de aplicar um algoritmo de agrupamento na seguinte base de dados:\n",
    "https://www02.smt.ufrj.br/~offshore/mfs/page_01.html\n",
    "Nela foram feitas algumas medições de vibrações do rotor do motor a 50 Hz. A estrutura de pasta que foi montada segue o seguinte arquivo anexado. Gostaria de primeiramente carregar esses dados usando polars dataframes\n",
    "\n",
    "-----------------\n",
    "\n",
    "Para carregar esses dados usando Polars DataFrames, você pode criar uma função que percorra as pastas e leia os arquivos CSV correspondentes. Aqui está um exemplo de código em Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "\n",
    "# Função para carregar os dados\n",
    "def load_vibration_data(base_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    # Percorrer as pastas\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Ler o arquivo CSV\n",
    "                df = pl.read_csv(file_path)\n",
    "                # Adicionar os dados ao dataframe principal\n",
    "                data.append(df)\n",
    "                # Extrair o label com base no nome do diretório\n",
    "                label = os.path.basename(os.path.dirname(file_path))\n",
    "                labels.append(label)\n",
    "    \n",
    "    # Combinar os dados em um único dataframe\n",
    "    combined_df = pl.concat(data)\n",
    "    return combined_df, labels\n",
    "\n",
    "# Caminho base para os dados\n",
    "base_path = \"Data\"  # Substitua pelo caminho correto dos seus dados\n",
    "\n",
    "# Carregar os dados\n",
    "combined_df, labels = load_vibration_data(base_path)\n",
    "\n",
    "# Exibir as primeiras linhas\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados consistem em séries temporais de vibrações dos motores. Há alguma forma de gerar atributos que consigam caracterizar bem os dados para aplicar um algoritmo de agrupamento capaz de detectar quando há uma anomalia?\n",
    "\n",
    "------------------------\n",
    "\n",
    "Sim, para caracterizar séries temporais de vibrações de motores e aplicar algoritmos de agrupamento para detectar anomalias, você pode gerar atributos derivados das séries temporais que capturem padrões relevantes para análise. Abaixo estão algumas categorias de atributos úteis, além de métodos para extraí-los:\n",
    "\n",
    "1. Estatísticas Básicas\n",
    "Média, mediana, variância e desvio padrão da amplitude.\n",
    "Amplitude máxima e mínima.\n",
    "Curtose e assimetria (skewness).\n",
    "Energia da série temporal.\n",
    "Exemplo em Polars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def basic_statistics(df):\n",
    "    stats = df.select([\n",
    "        pl.col(\"signal\").mean().alias(\"mean\"),\n",
    "        pl.col(\"signal\").std().alias(\"std_dev\"),\n",
    "        pl.col(\"signal\").min().alias(\"min\"),\n",
    "        pl.col(\"signal\").max().alias(\"max\"),\n",
    "        pl.col(\"signal\").kurtosis().alias(\"kurtosis\"),\n",
    "        pl.col(\"signal\").skew().alias(\"skewness\")\n",
    "    ])\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Atributos no Domínio da Frequência\n",
    "Transformar os dados para o domínio da frequência pode revelar padrões ocultos associados a vibrações específicas:\n",
    "\n",
    "Frequências dominantes: Componentes de maior magnitude no espectro.\n",
    "Energia em bandas específicas: Soma da potência em faixas de frequência (e.g., baixas, médias, altas).\n",
    "Transformada de Fourier (FFT): Frequência fundamental e harmônicas.\n",
    "Exemplo com scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "def frequency_features(signal, sampling_rate):\n",
    "    N = len(signal)\n",
    "    freq = np.fft.fftfreq(N, d=1/sampling_rate)\n",
    "    fft_values = np.abs(fft(signal))\n",
    "    \n",
    "    # Seleção de características\n",
    "    dominant_freq = freq[np.argmax(fft_values)]\n",
    "    energy = np.sum(fft_values**2)  # Energia total\n",
    "    return {\n",
    "        \"dominant_frequency\": dominant_freq,\n",
    "        \"frequency_energy\": energy\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Características Derivadas do Tempo\n",
    "Capturam comportamentos ao longo do tempo:\n",
    "\n",
    "Autocorrelação: Mede a relação do sinal consigo mesmo em diferentes atrasos (lags).\n",
    "Entropia de Permutação: Avalia a complexidade do sinal.\n",
    "Taxa de cruzamento de zero: Frequência com que o sinal cruza o zero (relacionado à oscilação).\n",
    "Exemplo de cruzamento de zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def zero_crossing_rate(signal):\n",
    "    zero_crossings = np.where(np.diff(np.sign(signal)))[0]\n",
    "    return len(zero_crossings) / len(signal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Características no Domínio do Tempo-Frequência\n",
    "Transformada Wavelet: Identifica padrões transitórios.\n",
    "Espectrograma: Representação de frequência ao longo do tempo. Essas técnicas são úteis para sinais com características que mudam no tempo.\n",
    "\n",
    "5. Técnicas Avançadas\n",
    "Redução de Dimensionalidade: Usar PCA (Principal Component Analysis) para reduzir a dimensionalidade das características derivadas.\n",
    "Aprendizado de Representação:\n",
    "Redes Neurais (e.g., autoencoders).\n",
    "Extração de embeddings usando técnicas de aprendizado profundo.\n",
    "\n",
    "Pipeline para Detectar Anomalias\n",
    "Pré-processamento: Normalizar os dados e lidar com ruído.\n",
    "Geração de Atributos: Usar as categorias acima para transformar os dados.\n",
    "Agrupamento: Aplicar algoritmos como K-Means, DBSCAN ou Isolation Forest nos atributos derivados.\n",
    "Validação: Avaliar os resultados manualmente ou com labels de anomalias, caso existam.\n",
    "Se quiser, posso implementar parte desse pipeline em Python para os seus dados, como a extração de atributos estatísticos ou baseados em frequência."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sim por favor, faça a implementação completa\n",
    "\n",
    "--------------------------\n",
    "\n",
    "Aqui está a implementação completa de um pipeline para caracterizar séries temporais de vibrações e aplicar algoritmos de agrupamento para detectar anomalias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Step 1: Load Data\n",
    "def load_vibration_data(base_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                df = pl.read_csv(file_path)\n",
    "                data.append(df)\n",
    "                label = os.path.basename(os.path.dirname(file_path))\n",
    "                labels.append(label)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "# Step 2: Extract Features\n",
    "def extract_features(df, sampling_rate):\n",
    "    signal = df[\"signal\"].to_numpy()\n",
    "\n",
    "    # Basic Statistics\n",
    "    mean = np.mean(signal)\n",
    "    std_dev = np.std(signal)\n",
    "    min_val = np.min(signal)\n",
    "    max_val = np.max(signal)\n",
    "    kurtosis = pl.Series(signal).kurtosis()\n",
    "    skewness = pl.Series(signal).skew()\n",
    "\n",
    "    # Frequency Domain Features\n",
    "    N = len(signal)\n",
    "    freq = np.fft.fftfreq(N, d=1 / sampling_rate)\n",
    "    fft_values = np.abs(fft(signal))\n",
    "    dominant_freq = freq[np.argmax(fft_values)]\n",
    "    energy = np.sum(fft_values**2)\n",
    "\n",
    "    # Time Domain Features\n",
    "    zero_crossings = np.where(np.diff(np.sign(signal)))[0]\n",
    "    zero_crossing_rate = len(zero_crossings) / len(signal)\n",
    "\n",
    "    return {\n",
    "        \"mean\": mean,\n",
    "        \"std_dev\": std_dev,\n",
    "        \"min\": min_val,\n",
    "        \"max\": max_val,\n",
    "        \"kurtosis\": kurtosis,\n",
    "        \"skewness\": skewness,\n",
    "        \"dominant_frequency\": dominant_freq,\n",
    "        \"frequency_energy\": energy,\n",
    "        \"zero_crossing_rate\": zero_crossing_rate,\n",
    "    }\n",
    "\n",
    "\n",
    "# Step 3: Build Feature Matrix\n",
    "def build_feature_matrix(data, sampling_rate):\n",
    "    features = []\n",
    "    for df in data:\n",
    "        features.append(extract_features(df, sampling_rate))\n",
    "\n",
    "    return pl.DataFrame(features)\n",
    "\n",
    "\n",
    "# Step 4: Preprocessing and Clustering\n",
    "def cluster_data(features):\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "    # Dimensionality Reduction (optional)\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_features = pca.fit_transform(scaled_features)\n",
    "\n",
    "    # DBSCAN Clustering\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    clusters = dbscan.fit_predict(reduced_features)\n",
    "\n",
    "    return reduced_features, clusters\n",
    "\n",
    "\n",
    "# Step 5: Visualize Clusters\n",
    "def visualize_clusters(reduced_features, clusters):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=clusters, cmap=\"viridis\")\n",
    "    plt.colorbar(label=\"Cluster Label\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.title(\"DBSCAN Clustering Results\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Main Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Base path to data\n",
    "    base_path = \"motorvibration/Data\"  # Update with the correct path\n",
    "\n",
    "    # Sampling rate (assumed, update as needed)\n",
    "    sampling_rate = 50\n",
    "\n",
    "    # Load Data\n",
    "    data, labels = load_vibration_data(base_path)\n",
    "    print(data)\n",
    "\n",
    "    # Build Feature Matrix\n",
    "    features = build_feature_matrix(data, sampling_rate)\n",
    "\n",
    "    # Clustering\n",
    "    reduced_features, clusters = cluster_data(features)\n",
    "\n",
    "    # Visualize Results\n",
    "    visualize_clusters(reduced_features, clusters)\n",
    "\n",
    "    # Display cluster assignment\n",
    "    print(\"Cluster Assignments:\", clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada arquivo csv apresenta 8 colunas, elas apontam as medições na imagem. Portanto as colunas serão:\n",
    "dataset_columns_name = [\"tachometer\", \"underhang_1\", \"underhang_2\", \"underhang_3\", \"overhang_1\", \"overhang_2\", \"overhang_3\", \"microphone\"]\n",
    "Poderia também colocar uma barra de carregamento para indicar que a base está sendo carregada? \n",
    "\n",
    "--------------\n",
    "\n",
    "Adicionei uma barra de carregamento usando a biblioteca tqdm para indicar o progresso do carregamento dos arquivos e da extração de características. Além disso, atualizei as colunas do dataset para refletir os nomes fornecidos. Certifique-se de que a biblioteca tqdm esteja instalada (pip install tqdm) antes de executar o código. Se precisar de ajustes adicionais, é só avisar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Load Data\n",
    "def load_vibration_data(base_path):\n",
    "    dataset_columns_name = [\"tachometer\", \"underhang_1\", \"underhang_2\", \"underhang_3\", \"overhang_1\", \"overhang_2\", \"overhang_3\", \"microphone\"]\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in tqdm(files, desc=\"Loading files\"):\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                df = pl.read_csv(file_path, has_header=True, new_columns=dataset_columns_name)\n",
    "                data.append(df)\n",
    "                label = os.path.basename(os.path.dirname(file_path))\n",
    "                labels.append(label)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Step 2: Extract Features\n",
    "def extract_features(df, sampling_rate):\n",
    "    signal = df[\"microphone\"].to_numpy()\n",
    "\n",
    "    # Basic Statistics\n",
    "    mean = np.mean(signal)\n",
    "    std_dev = np.std(signal)\n",
    "    min_val = np.min(signal)\n",
    "    max_val = np.max(signal)\n",
    "    kurtosis = pl.Series(signal).kurtosis()\n",
    "    skewness = pl.Series(signal).skew()\n",
    "\n",
    "    # Frequency Domain Features\n",
    "    N = len(signal)\n",
    "    freq = np.fft.fftfreq(N, d=1 / sampling_rate)\n",
    "    fft_values = np.abs(fft(signal))\n",
    "    dominant_freq = freq[np.argmax(fft_values)]\n",
    "    energy = np.sum(fft_values ** 2)\n",
    "\n",
    "    # Time Domain Features\n",
    "    zero_crossings = np.where(np.diff(np.sign(signal)))[0]\n",
    "    zero_crossing_rate = len(zero_crossings) / len(signal)\n",
    "\n",
    "    return {\n",
    "        \"mean\": mean,\n",
    "        \"std_dev\": std_dev,\n",
    "        \"min\": min_val,\n",
    "        \"max\": max_val,\n",
    "        \"kurtosis\": kurtosis,\n",
    "        \"skewness\": skewness,\n",
    "        \"dominant_frequency\": dominant_freq,\n",
    "        \"frequency_energy\": energy,\n",
    "        \"zero_crossing_rate\": zero_crossing_rate,\n",
    "    }\n",
    "\n",
    "# Step 3: Build Feature Matrix\n",
    "def build_feature_matrix(data, sampling_rate):\n",
    "    features = []\n",
    "    for df in tqdm(data, desc=\"Extracting features\"):\n",
    "        features.append(extract_features(df, sampling_rate))\n",
    "\n",
    "    return pl.DataFrame(features)\n",
    "\n",
    "# Step 4: Preprocessing and Clustering\n",
    "def cluster_data(features):\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "    # Dimensionality Reduction (optional)\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_features = pca.fit_transform(scaled_features)\n",
    "\n",
    "    # DBSCAN Clustering\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    clusters = dbscan.fit_predict(reduced_features)\n",
    "\n",
    "    return reduced_features, clusters\n",
    "\n",
    "# Step 5: Visualize Clusters\n",
    "def visualize_clusters(reduced_features, clusters):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=clusters, cmap=\"viridis\")\n",
    "    plt.colorbar(label=\"Cluster Label\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.title(\"DBSCAN Clustering Results\")\n",
    "    plt.show()\n",
    "\n",
    "# Main Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Base path to data\n",
    "    base_path = \"Data\"  # Update with the correct path\n",
    "\n",
    "    # Sampling rate (update as needed)\n",
    "    sampling_rate = 50000\n",
    "\n",
    "    # Load Data\n",
    "    data, labels = load_vibration_data(base_path)\n",
    "\n",
    "    # Build Feature Matrix\n",
    "    features = build_feature_matrix(data, sampling_rate)\n",
    "\n",
    "    # Clustering\n",
    "    reduced_features, clusters = cluster_data(features)\n",
    "\n",
    "    # Visualize Results\n",
    "    visualize_clusters(reduced_features, clusters)\n",
    "\n",
    "    # Display cluster assignment\n",
    "    print(\"Cluster Assignments:\", clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementei a seguinte função para facilitar a listagem dos diversos arquivos que existem:\n",
    "\n",
    "import os\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "# Step 1: List Files\n",
    "def list_files(dataset_path):\n",
    "    csv_files_list = []\n",
    "\n",
    "    folders_list = os.listdir(dataset_path)\n",
    "    for folder in folders_list:\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            sub_folders_list = os.listdir(folder_path)\n",
    "            for sub_folder in sub_folders_list:\n",
    "                sub_folder_path = os.path.join(folder_path, sub_folder)\n",
    "                if os.path.isdir(sub_folder_path):\n",
    "                    list_csv = glob(f\"{sub_folder_path}/*.csv\")\n",
    "                    for csv_file in list_csv:\n",
    "                        csv_files_list.append((csv_file, csv_file))\n",
    "                elif sub_folder_path.endswith(\".csv\"):\n",
    "                    csv_files_list.append((sub_folder_path, sub_folder_path))\n",
    "\n",
    "    return csv_files_list\n",
    "\n",
    "# Step 2: Load Data\n",
    "def load_vibration_data(base_path):\n",
    "    dataset_columns_name = [\"tachometer\", \"underhang_1\", \"underhang_2\", \"underhang_3\", \"overhang_1\", \"overhang_2\", \"overhang_3\", \"microphone\"]\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in tqdm(files, desc=\"Loading files\"):\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                df = pl.read_csv(file_path, has_header=True, new_columns=dataset_columns_name)\n",
    "                data.append(df)\n",
    "                label = os.path.basename(os.path.dirname(file_path))\n",
    "                labels.append(label)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Step 3: Extract Features\n",
    "def extract_features(df, sampling_rate):\n",
    "    signal = df[\"microphone\"].to_numpy()\n",
    "\n",
    "    # Basic Statistics\n",
    "    mean = np.mean(signal)\n",
    "    std_dev = np.std(signal)\n",
    "    min_val = np.min(signal)\n",
    "    max_val = np.max(signal)\n",
    "    kurtosis = pl.Series(signal).kurtosis()\n",
    "    skewness = pl.Series(signal).skew()\n",
    "\n",
    "    # Frequency Domain Features\n",
    "    N = len(signal)\n",
    "    freq = np.fft.fftfreq(N, d=1 / sampling_rate)\n",
    "    fft_values = np.abs(fft(signal))\n",
    "    dominant_freq = freq[np.argmax(fft_values)]\n",
    "    energy = np.sum(fft_values ** 2)\n",
    "\n",
    "    # Time Domain Features\n",
    "    zero_crossings = np.where(np.diff(np.sign(signal)))[0]\n",
    "    zero_crossing_rate = len(zero_crossings) / len(signal)\n",
    "\n",
    "    return {\n",
    "        \"mean\": mean,\n",
    "        \"std_dev\": std_dev,\n",
    "        \"min\": min_val,\n",
    "        \"max\": max_val,\n",
    "        \"kurtosis\": kurtosis,\n",
    "        \"skewness\": skewness,\n",
    "        \"dominant_frequency\": dominant_freq,\n",
    "        \"frequency_energy\": energy,\n",
    "        \"zero_crossing_rate\": zero_crossing_rate,\n",
    "    }\n",
    "\n",
    "# Step 4: Build Feature Matrix\n",
    "def build_feature_matrix(data, sampling_rate):\n",
    "    features = []\n",
    "    for df in tqdm(data, desc=\"Extracting features\"):\n",
    "        features.append(extract_features(df, sampling_rate))\n",
    "\n",
    "    return pl.DataFrame(features)\n",
    "\n",
    "# Step 5: Preprocessing and Clustering\n",
    "def cluster_data(features):\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "    # Dimensionality Reduction (optional)\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_features = pca.fit_transform(scaled_features)\n",
    "\n",
    "    # DBSCAN Clustering\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    clusters = dbscan.fit_predict(reduced_features)\n",
    "\n",
    "    return reduced_features, clusters\n",
    "\n",
    "# Step 6: Visualize Clusters\n",
    "def visualize_clusters(reduced_features, clusters):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=clusters, cmap=\"viridis\")\n",
    "    plt.colorbar(label=\"Cluster Label\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.title(\"DBSCAN Clustering Results\")\n",
    "    plt.show()\n",
    "\n",
    "# Main Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Base path to data\n",
    "    base_path = \"Data\"  # Update with the correct path\n",
    "\n",
    "    # Sampling rate (update as needed)\n",
    "    sampling_rate = 50000\n",
    "\n",
    "    # List Files\n",
    "    file_list = list_files(base_path)\n",
    "    print(\"Files found:\", file_list)\n",
    "\n",
    "    # Load Data\n",
    "    data, labels = load_vibration_data(base_path)\n",
    "\n",
    "    # Build Feature Matrix\n",
    "    features = build_feature_matrix(data, sampling_rate)\n",
    "\n",
    "    # Clustering\n",
    "    reduced_features, clusters = cluster_data(features)\n",
    "\n",
    "    # Visualize Results\n",
    "    visualize_clusters(reduced_features, clusters)\n",
    "\n",
    "    # Display cluster assignment\n",
    "    print(\"Cluster Assignments:\", clusters)\n",
    "\n",
    "--------\n",
    "\n",
    "Adicionei a função list_files que você forneceu ao código existente e integrei sua utilização no pipeline principal para listar os arquivos antes de carregar os dados. Também inclui a exibição dos arquivos encontrados para facilitar a visualização. Se precisar de mais ajustes ou aprimoramentos, é só avisar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "# Step 1: List Files\n",
    "def list_files(dataset_path):\n",
    "    csv_files_list = []\n",
    "\n",
    "    folders_list = os.listdir(dataset_path)\n",
    "    for folder in folders_list:\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            sub_folders_list = os.listdir(folder_path)\n",
    "            for sub_folder in sub_folders_list:\n",
    "                sub_folder_path = os.path.join(folder_path, sub_folder)\n",
    "                if os.path.isdir(sub_folder_path):\n",
    "                    list_csv = glob(f\"{sub_folder_path}/*.csv\")\n",
    "                    for csv_file in list_csv:\n",
    "                        csv_files_list.append((csv_file, csv_file))\n",
    "                elif sub_folder_path.endswith(\".csv\"):\n",
    "                    csv_files_list.append((sub_folder_path, sub_folder_path))\n",
    "\n",
    "    return csv_files_list\n",
    "\n",
    "# Step 2: Load Data\n",
    "def load_vibration_data(base_path):\n",
    "    dataset_columns_name = [\"tachometer\", \"underhang_1\", \"underhang_2\", \"underhang_3\", \"overhang_1\", \"overhang_2\", \"overhang_3\", \"microphone\"]\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in tqdm(files, desc=\"Loading files\"):\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                df = pl.read_csv(file_path, has_header=True, new_columns=dataset_columns_name)\n",
    "                data.append(df)\n",
    "                label = os.path.basename(os.path.dirname(file_path))\n",
    "                labels.append(label)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Step 3: Extract Features\n",
    "def extract_features(df, sampling_rate):\n",
    "    signal = df[\"microphone\"].to_numpy()\n",
    "\n",
    "    # Basic Statistics\n",
    "    mean = np.mean(signal)\n",
    "    std_dev = np.std(signal)\n",
    "    min_val = np.min(signal)\n",
    "    max_val = np.max(signal)\n",
    "    kurtosis = pl.Series(signal).kurtosis()\n",
    "    skewness = pl.Series(signal).skew()\n",
    "\n",
    "    # Frequency Domain Features\n",
    "    N = len(signal)\n",
    "    freq = np.fft.fftfreq(N, d=1 / sampling_rate)\n",
    "    fft_values = np.abs(fft(signal))\n",
    "    dominant_freq = freq[np.argmax(fft_values)]\n",
    "    energy = np.sum(fft_values ** 2)\n",
    "\n",
    "    # Time Domain Features\n",
    "    zero_crossings = np.where(np.diff(np.sign(signal)))[0]\n",
    "    zero_crossing_rate = len(zero_crossings) / len(signal)\n",
    "\n",
    "    return {\n",
    "        \"mean\": mean,\n",
    "        \"std_dev\": std_dev,\n",
    "        \"min\": min_val,\n",
    "        \"max\": max_val,\n",
    "        \"kurtosis\": kurtosis,\n",
    "        \"skewness\": skewness,\n",
    "        \"dominant_frequency\": dominant_freq,\n",
    "        \"frequency_energy\": energy,\n",
    "        \"zero_crossing_rate\": zero_crossing_rate,\n",
    "    }\n",
    "\n",
    "# Step 4: Build Feature Matrix\n",
    "def build_feature_matrix(data, sampling_rate):\n",
    "    features = []\n",
    "    for df in tqdm(data, desc=\"Extracting features\"):\n",
    "        features.append(extract_features(df, sampling_rate))\n",
    "\n",
    "    return pl.DataFrame(features)\n",
    "\n",
    "# Step 5: Preprocessing and Clustering\n",
    "def cluster_data(features):\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "    # Dimensionality Reduction (optional)\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_features = pca.fit_transform(scaled_features)\n",
    "\n",
    "    # DBSCAN Clustering\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    clusters = dbscan.fit_predict(reduced_features)\n",
    "\n",
    "    return reduced_features, clusters\n",
    "\n",
    "# Step 6: Visualize Clusters\n",
    "def visualize_clusters(reduced_features, clusters):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=clusters, cmap=\"viridis\")\n",
    "    plt.colorbar(label=\"Cluster Label\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.title(\"DBSCAN Clustering Results\")\n",
    "    plt.show()\n",
    "\n",
    "# Main Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Base path to data\n",
    "    base_path = \"Data\"  # Update with the correct path\n",
    "\n",
    "    # Sampling rate (update as needed)\n",
    "    sampling_rate = 50000\n",
    "\n",
    "    # List Files\n",
    "    file_list = list_files(base_path)\n",
    "    print(\"Files found:\", file_list)\n",
    "\n",
    "    # Load Data\n",
    "    data, labels = load_vibration_data(base_path)\n",
    "\n",
    "    # Build Feature Matrix\n",
    "    features = build_feature_matrix(data, sampling_rate)\n",
    "\n",
    "    # Clustering\n",
    "    reduced_features, clusters = cluster_data(features)\n",
    "\n",
    "    # Visualize Results\n",
    "    visualize_clusters(reduced_features, clusters)\n",
    "\n",
    "    # Display cluster assignment\n",
    "    print(\"Cluster Assignments:\", clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poderia gerar um segundo plot no final, onde são apontados os rótulos, nas listas retornadas pelo list_files há tuplas elas indicam:\n",
    "(caminho_do_arquivo, nome_do_arquivo). Todo arquivo que tenha a palavra \"normal\"  é uma medição normal, caso contrário, é uma medição onde há anomalia\n",
    "\n",
    "--------\n",
    "\n",
    "Adicionei um segundo gráfico que classifica os arquivos como \"normal\" ou \"anomaly\" com base no nome do arquivo e os exibe usando diferentes cores. Certifique-se de que o conjunto de dados contém os nomes esperados para que os rótulos sejam atribuídos corretamente. Se precisar de mais ajustes, é só avisar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "# Step 1: List Files\n",
    "def list_files(dataset_path):\n",
    "    csv_files_list = []\n",
    "\n",
    "    folders_list = os.listdir(dataset_path)\n",
    "    for folder in folders_list:\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            sub_folders_list = os.listdir(folder_path)\n",
    "            for sub_folder in sub_folders_list:\n",
    "                sub_folder_path = os.path.join(folder_path, sub_folder)\n",
    "                if os.path.isdir(sub_folder_path):\n",
    "                    list_csv = glob(f\"{sub_folder_path}/*.csv\")\n",
    "                    for csv_file in list_csv:\n",
    "                        csv_files_list.append((csv_file, csv_file))\n",
    "                elif sub_folder_path.endswith(\".csv\"):\n",
    "                    csv_files_list.append((sub_folder_path, sub_folder_path))\n",
    "\n",
    "    return csv_files_list\n",
    "\n",
    "# Step 2: Load Data\n",
    "def load_vibration_data_from_list(file_list):\n",
    "    dataset_columns_name = [\"tachometer\", \"underhang_1\", \"underhang_2\", \"underhang_3\", \"overhang_1\", \"overhang_2\", \"overhang_3\", \"microphone\"]\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for file_path, label in tqdm(file_list, desc=\"Loading files\"):\n",
    "        df = pl.read_csv(file_path, has_header=True, new_columns=dataset_columns_name)\n",
    "        data.append(df)\n",
    "        labels.append(label)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Step 3: Extract Features\n",
    "def extract_features(df, sampling_rate):\n",
    "    signal = df[\"microphone\"].to_numpy()\n",
    "\n",
    "    # Basic Statistics\n",
    "    mean = np.mean(signal)\n",
    "    std_dev = np.std(signal)\n",
    "    min_val = np.min(signal)\n",
    "    max_val = np.max(signal)\n",
    "    kurtosis = pl.Series(signal).kurtosis()\n",
    "    skewness = pl.Series(signal).skew()\n",
    "\n",
    "    # Frequency Domain Features\n",
    "    N = len(signal)\n",
    "    freq = np.fft.fftfreq(N, d=1 / sampling_rate)\n",
    "    fft_values = np.abs(fft(signal))\n",
    "    dominant_freq = freq[np.argmax(fft_values)]\n",
    "    energy = np.sum(fft_values ** 2)\n",
    "\n",
    "    # Time Domain Features\n",
    "    zero_crossings = np.where(np.diff(np.sign(signal)))[0]\n",
    "    zero_crossing_rate = len(zero_crossings) / len(signal)\n",
    "\n",
    "    return {\n",
    "        \"mean\": mean,\n",
    "        \"std_dev\": std_dev,\n",
    "        \"min\": min_val,\n",
    "        \"max\": max_val,\n",
    "        \"kurtosis\": kurtosis,\n",
    "        \"skewness\": skewness,\n",
    "        \"dominant_frequency\": dominant_freq,\n",
    "        \"frequency_energy\": energy,\n",
    "        \"zero_crossing_rate\": zero_crossing_rate,\n",
    "    }\n",
    "\n",
    "# Step 4: Build Feature Matrix\n",
    "def build_feature_matrix(data, sampling_rate):\n",
    "    features = []\n",
    "    for df in tqdm(data, desc=\"Extracting features\"):\n",
    "        features.append(extract_features(df, sampling_rate))\n",
    "\n",
    "    return pl.DataFrame(features)\n",
    "\n",
    "# Step 5: Preprocessing and Clustering\n",
    "def cluster_data(features):\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "    # Dimensionality Reduction (optional)\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_features = pca.fit_transform(scaled_features)\n",
    "\n",
    "    # DBSCAN Clustering\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    clusters = dbscan.fit_predict(reduced_features)\n",
    "\n",
    "    return reduced_features, clusters\n",
    "\n",
    "# Step 6: Visualize Clusters\n",
    "def visualize_clusters(reduced_features, clusters):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=clusters, cmap=\"viridis\")\n",
    "    plt.colorbar(label=\"Cluster Label\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.title(\"DBSCAN Clustering Results\")\n",
    "    plt.show()\n",
    "\n",
    "def visualize_labels(file_list, clusters):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    labels = [\"normal\" if \"normal\" in name.lower() else \"anomaly\" for _, name in file_list]\n",
    "    color_map = {\"normal\": \"blue\", \"anomaly\": \"red\"}\n",
    "    colors = [color_map[label] for label in labels]\n",
    "\n",
    "    plt.scatter(clusters[:, 0], clusters[:, 1], c=colors, label=labels, alpha=0.6)\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.title(\"Visualization by Labels (Normal vs Anomaly)\")\n",
    "    plt.show()\n",
    "\n",
    "# Main Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Base path to data\n",
    "    base_path = \"Data\"  # Update with the correct path\n",
    "\n",
    "    # Sampling rate (update as needed)\n",
    "    sampling_rate = 50000\n",
    "\n",
    "    # List Files\n",
    "    file_list = list_files(base_path)\n",
    "    print(\"Files found:\", file_list)\n",
    "\n",
    "    # Load Data\n",
    "    data, labels = load_vibration_data_from_list(file_list)\n",
    "\n",
    "    # Build Feature Matrix\n",
    "    features = build_feature_matrix(data, sampling_rate)\n",
    "\n",
    "    # Clustering\n",
    "    reduced_features, clusters = cluster_data(features)\n",
    "\n",
    "    # Visualize Results\n",
    "    visualize_clusters(reduced_features, clusters)\n",
    "\n",
    "    # Second Plot: Visualization by Labels\n",
    "    visualize_labels(file_list, reduced_features)\n",
    "\n",
    "    # Display cluster assignment\n",
    "    print(\"Cluster Assignments:\", clusters)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
